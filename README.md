# DS Projen

TODOs

- [ ] Actually test the GitHub actions workflow (see the `reference-migration` branch of `data-science-projects`).
  - [ ] Manually
  - [ ] Via an automation; we'll need to create a repo and set of outerbounds users
        for this. Painful but vital.
- [ ] Add boilerplate sample tests to the MetaflowProject
  - [ ] Include `conftest.py`
  - [ ] Include a subfoler with PYTHONPATH hacking so reusable imports
        can be defined during tests
- [ ] Make it so `pre-commit` doesn't fail due to changes to files autogenerated by projen
  - [ ] Find a list of the projen managed files in `.projen/files.json`
    - [ ] Option: add an exclude statement in `pre-commit-config.yaml`
          and automatically add all projen-managed files
    - [ ] Option: put projen-managed files in a `projen-files.txt` and cat them
          into the `pre-commit run ...` command. Not sure how this would work
          for `git commit ...`. I wish `.pre-commit-ignore` files were a thing.
- [ ] Get test coverage up to 90%.
  - [ ] Add unit tests for
    - [ ] Individual components
  - [ ] Add functional tests for
    - [ ] A task runner, e.g. run lint and validate that it catches/fixes errors
    - [ ] Run generated tests


```python
# Sample .projenrc.py file

from ds_projen import Domain, MetaflowProject, Repository

repo = Repository(name="data-science-projects")

project = MetaflowProject(
    repo=repo,
    name="nlp-project",
    description="Test DS project",
    version="0.0.0",
    domain=Domain.CONTENT,
    dependencies=["numpy", "pandas"],
    dev_dependencies=["pytest"],
)

project.add_flow(
    flow_name="nlp_flow",
    relative_flow_path="nlp_flow.py",
    use_conda_base=True,
    python_version="3.11",
    packages={"numpy": "1.23.5", "pandas": "1.4.2"},
)

_ = MetaflowProject(
    repo=repo,
    name="new-project",
    description="Test DS project2",
    domain=Domain.ADVERTISING,
)

repo.synth()
```


// TODO: Clean up the README


```sh
# Monorepo directory structure

data-science-projects
├── .github
│   ├── workflows
│   │   ├── {{ project-ci-cd }}.yaml
│   │   ├── code-quality-checks.yaml
│   │   └── terraform-ci-cd-prod.yml
│   └── CODEOWNERS
├── deployment/
├── docs/
├── domains
│   ├── advertising
│   │   └── .gitkeep
│   ├── advisory
│   │   ├── attribute_analysis/
│   ├── content
│   │   ├── niche-insights/
│   │   └── parallel-flows/
│   ├── demand_generation
│   │   └── .gitkeep
│   ├── forecasting
│   │   └── .gitkeep
│   ├── market_intelligence
│   │   ├── buybox_predictor/
│   └── operations
│       └── .gitkeep
├── .gitignore
├── .pre-commit-config.yaml
├── README.md
├── pyproject.toml
└── run
```


I'd like to create a projen library with the following abstraction:

Should be in `patterninc/ds-projen`

1. A class called Repository which represents the monorepo

    - It should have a list of `MetaflowProject`s

2. A class called `MetaflowProject`, it should have

   - a param `domain: Literal["content", "reference", ...the rest of the domains]` __init__() should fail if the domain entered is not a valid one

   - `project_name: str` e.g. `niche-insights` which would result in it being placed at `domains/content/niche-insights`

   - a method `forecasting_project.add_flow()` with params
    
      - `relative_flow_path: str | Path` e.g. "path/to/some_flow.py" which would result in domains/content/niche-insights/path/to/some_flow.py being created as a sample file aka `domains/{domain}/{project_name}/{rel_fpath}`

      - a param called `runs_in_ci: bool` which causes the flow to be executed in CI (the python flow.py run command)

      - a param called `config_rel_fpath_ci: str | Path | None`
      
      - `config_rel_fpath_non_prod: str | Path | None`

      - `config_rel_fpath_prod: str | Path | None` -- each of these determine which path if any gets passed to `python myflow.py ... --config ./configs/{config_rel_fpath}`. The path is relative to the `./configs/` folder
  

  - The project should have a global `.github/workflows/<domain>-<project name>-ci-cd.yaml` with the following

    - if it's a PR, run tests, linting, projen synth

    <!-- What's a perimeter? What's the difference between prod and non-prod?  -->
    - trigger a deploy to the default perimeter   

    - if it's merge to main, trigger a deploy to the prod perimeter

    - it should deploy all the flows in parallel using a github actions matrix

    - Also, add a workflow_dispatch that allows you manually trigger a deploy to



```python
from ds_projen import MetaflowProject, Repository

# Define the monorepo
repo = Repository(name="data-science-projects")

# Create a project
nlp_project = MetaflowProject(
    domain=DOMAINS.CONTENT,
    project_name="nlp-project",
    import_module_name="nlp_project",   # defaults to project_name replaced with "_"
    python_version="3.11",              # defaults to None, uv will choose it
    dependencies=["numpy", "pandas"],
    dev_dependencies=["pytest", "black"],
    description="A project for NLP tasks",
    authors=[{"name": "Amit", "email": "amit.raj@pattern.com"}],
)

# Add a flow to the project
nlp_project.add_flow(
    flow_name="NLPFlow",                                      # name of the flow
    relative_flow_path="flow.py",                             # relative to the project root
    use_pypi_base=True,                                       # bool, if True, pypi_base decorator will be used (default: True)
    use_conda_base=None,                                      # bool, if True, conda_base decorator will be used (default: None)
    # runs_in_ci=True,
    # prod_config_relative_file_path="configs/prod.json",       # used in CI for deploying the flow i.e., creating an Agro workflow
    # non_prod_config_relative_file_path="configs/dev.json",    # can be used in CI to test run the flow; flow will run with `--with kubernetes` flag
    # config_relative_file_path_ci="configs/dev.json",          # if provided, flow will run in CI with this config
)
# Add gitignore
repo.gitignore.add_patterns(
    "/uv.lock",  # ignore the top-level uv.lock file
    ".metaflow",  # ignore the metaflow folder
    "metaflow.s3.*", # ignore the metaflow.s3 artifacts
    "**/.DS_Store",  # ignore macOS system files
)

# Synthesize the project
repo.synth()
```

```shell
# Setting up a Metaflow Project:

.github/workflows/nlp-project.yaml

domains/content/nlp-project
├── .gitignore
├── configs                  # lazy sample file --  will not be managed by projen after creation
│   └── dev.json
│   └── prod.json
├── nlp_project
│   ├── __init__.py
│   └── module.py
├── sql                     # projen will not create this folder
│   └── sql_file.sql
├── tests
│   └── test__module.py
├── README.md               # lazy sample file --  will not be managed by projen after creation
├── flow.py                 # lazy sample file --  will not be managed by projen after creation
├── pyproject.toml
└── uv.lock                 # Automatically generated by uv -- not managed by projen
```


```toml
# Sample pyproject.toml file
[project]
name = "{{ project_name }}"
version = "{{ version }}"
description = "{{ description }}"
readme = "README.md"
requires-python = " {{ python_version }} "
dependencies = {{ list_of_dependencies }}

[dependency-groups]
dev = {{ list_of_dev_dependencies }}

# adding python path for pytest
[tool.pytest.ini_options]
pythonpath = ["."]
```


```python
# Sample metaflow py file

from metaflow import Config, FlowSpec, pypi_base, step


@pypi_base(
    python={{ python_version }},
    packages={{ dict_of_dependencies }},
)
class {{ flow_name }}(FlowSpec):
    """A sample flow for {{ project_name }}."""

    config = Config(name="config", default={{ non_prod_config_relative_file_path }})

    @step
    def start(self):
        """Start the flow."""
        self.next(self.end)

    @step
    def end(self):
        """End the flow."""
        pass


if __name__ == "__main__":
    {{ flow_name }}()
```


```yaml
# Sample workflow file
name: {{ project_name }}-deploy
on:
    workflow_dispatch:
    push:
        branches:
            - main
        paths:
            - '{{ out_dir }}/{{ project_name }}/**'
    pull_request:
        types: [opened, synchronize]
        branches:
            - main
        paths:
            - '{{ out_dir }}/{{ project_name }}/**'

jobs:
    ds-project-deploy:
        name: Deploy {{ project_name }} to Outerbounds
        permissions:
            contents: read
            id-token: write

        env:
            WORKDIR: {{ out_dir }}/{{ project_name }}

        defaults:
          run:
            working-directory: ${{ env.WORKDIR }}

        runs-on: ubuntu-latest
        steps:
            - name: Checkout repository
              uses: actions/checkout@v4

            - name: Set up uv
              uses: astral-sh/setup-uv@v5
              with:
                python-version: '3.11.2'    # Optionally you can set the python version
                enable-cache: true
                cache-dependency-glob: "${{ env.WORKDIR }}/uv.lock"

            - name: Run Tests
              run: uv run pytest tests

            # Optionally you can run the same command with `uvx outerbounds ...`
            # though it's not necessary if outerbounds is your dependency
            - name: Configure Outerbounds
              run: |-
                uv outerbounds service-principal-configure \
                --name github-user-ds-projects \
                --deployment-domain pattern.obp.outerbounds.com \
                --perimeter default \
                --github-actions

            - name: Run a Dev Flow      # Optional Dev Flow
              if: github.ref == 'refs/heads/main' || github.event_name == 'pull_request'
              run: |-
                uv run {{ flow.py }} \
                    --config config {{ dev_config_path }} \
                    --environment=fast-bakery \
                    --package-suffixes='.csv' \
                    run \
                    --tag triggered-from-pr

            - name: Deploy Prod Flow
              if: github.ref == 'refs/heads/main'
              run: |-
                uv run {{ flow.py }} \
                    --config config {{ prod_config_path }} \
                    --environment=fast-bakery \
                    --package-suffixes='.csv' \
                    --production \
                    argo-workflows create
```

